"""
Module phÃ¢n cá»¥m vÃ  chá»n vÄƒn báº£n Ä‘áº¡i diá»‡n
"""
import numpy as np
from typing import List, Tuple, Dict
from collections import defaultdict


class UnionFind:
    """Cáº¥u trÃºc Union-Find Ä‘á»ƒ phÃ¢n cá»¥m"""
    
    def __init__(self, n: int):
        self.parent = list(range(n))
        self.rank = [0] * n
    
    def find(self, x: int) -> int:
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]
    
    def union(self, x: int, y: int):
        px, py = self.find(x), self.find(y)
        if px == py:
            return
        if self.rank[px] < self.rank[py]:
            px, py = py, px
        self.parent[py] = px
        if self.rank[px] == self.rank[py]:
            self.rank[px] += 1


def cluster_documents(
    pairs: List[Tuple[int, int, float]],
    n_docs: int
) -> Dict[int, List[int]]:
    """
    PhÃ¢n cá»¥m dá»±a trÃªn cÃ¡c cáº·p tÆ°Æ¡ng tá»±
    
    Args:
        pairs: List cÃ¡c cáº·p (doc_id1, doc_id2, similarity)
        n_docs: Tá»•ng sá»‘ vÄƒn báº£n
    
    Returns:
        Dict {cluster_root: [doc_ids]}
    """
    uf = UnionFind(n_docs)
    
    for i, j, _ in pairs:
        uf.union(i, j)
    
    clusters = defaultdict(list)
    for doc_id in range(n_docs):
        root = uf.find(doc_id)
        clusters[root].append(doc_id)
    
    # Chá»‰ giá»¯ clusters cÃ³ > 1 vÄƒn báº£n
    return {k: v for k, v in clusters.items() if len(v) > 1}


def select_representative(
    cluster: List[int],
    texts: List[str],
    embeddings: np.ndarray = None,
    method: str = 'shortest'
) -> int:
    """
    Chá»n vÄƒn báº£n Ä‘áº¡i diá»‡n cho cá»¥m
    
    Args:
        cluster: List doc_ids trong cá»¥m
        texts: Danh sÃ¡ch táº¥t cáº£ vÄƒn báº£n
        embeddings: Embeddings (optional, cho method='centroid')
        method: 'shortest', 'longest', 'centroid'
    
    Returns:
        Doc ID cá»§a vÄƒn báº£n Ä‘áº¡i diá»‡n
    """
    
    if len(cluster) == 1:
        return cluster[0]
    
    if method == 'centroid' and embeddings is not None:
        # Chá»n vÄƒn báº£n gáº§n centroid nháº¥t
        cluster_vecs = embeddings[cluster]
        centroid = np.mean(cluster_vecs, axis=0)
        distances = np.linalg.norm(cluster_vecs - centroid, axis=1)
        return cluster[np.argmin(distances)]
    
    elif method == 'shortest':
        # Chá»n vÄƒn báº£n ngáº¯n nháº¥t
        return min(cluster, key=lambda i: len(texts[i]))
    
    elif method == 'longest':
        # Chá»n vÄƒn báº£n dÃ i nháº¥t
        return max(cluster, key=lambda i: len(texts[i]))
    
    else:
        return cluster[0]


def process_clustering(
    pairs: List[Tuple[int, int, float]],
    texts: List[str],
    embeddings: np.ndarray = None,
    representative_method: str = 'centroid'
) -> Dict:
    """
    PhÃ¢n cá»¥m vÃ  chá»n Ä‘áº¡i diá»‡n cho má»—i cá»¥m
    
    Returns:
        {
            'clusters': {
                cluster_id: {
                    'docs': [doc_ids],
                    'representative': int,
                    'documents': [
                        {'id': int, 'text': str, 'is_representative': bool}
                    ]
                }
            },
            'stats': {...}
        }
    """
    
    n_docs = len(texts)
    print(f"\nğŸ”— PhÃ¢n cá»¥m: {n_docs} vÄƒn báº£n, {len(pairs)} cáº·p tÆ°Æ¡ng tá»±")
    
    # PhÃ¢n cá»¥m
    clusters_raw = cluster_documents(pairs, n_docs)
    print(f"   TÃ¬m Ä‘Æ°á»£c {len(clusters_raw)} cá»¥m trÃ¹ng láº·p")
    
    # Xá»­ lÃ½ tá»«ng cá»¥m
    clusters_output = {}
    all_duplicates = set()
    
    for cluster_id, doc_ids in clusters_raw.items():
        # Chá»n Ä‘áº¡i diá»‡n
        representative_id = select_representative(
            doc_ids, texts, embeddings, representative_method
        )
        
        # Táº¡o danh sÃ¡ch documents cho cluster
        documents = []
        for doc_id in doc_ids:
            documents.append({
                'id': doc_id,
                'text': texts[doc_id],
                'is_representative': (doc_id == representative_id)
            })
            
            if doc_id != representative_id:
                all_duplicates.add(doc_id)
        
        clusters_output[cluster_id] = {
            'docs': doc_ids,
            'representative': representative_id,
            'size': len(doc_ids),
            'documents': documents
        }
    
    # TÃ­nh thá»‘ng kÃª
    n_removed = len(all_duplicates)
    n_kept = n_docs - n_removed
    
    stats = {
        'total_docs': n_docs,
        'n_clusters': len(clusters_output),
        'n_removed': n_removed,
        'n_kept': n_kept,
        'n_pairs': len(pairs),
        'removal_rate': n_removed / n_docs if n_docs > 0 else 0
    }
    
    print(f"âœ“ Thá»‘ng kÃª:")
    print(f"   - Tá»•ng vÄƒn báº£n: {stats['total_docs']}")
    print(f"   - Sá»‘ cá»¥m: {stats['n_clusters']}")
    print(f"   - VÄƒn báº£n bá»‹ loáº¡i: {stats['n_removed']}")
    print(f"   - VÄƒn báº£n giá»¯ láº¡i: {stats['n_kept']}")
    print(f"   - Tá»· lá»‡ loáº¡i: {stats['removal_rate']:.1%}")
    
    return {
        'clusters': clusters_output,  # â† ÄÃ¢y pháº£i lÃ  dict
        'stats': stats,
        'duplicates': sorted(list(all_duplicates)),
        'kept': [i for i in range(n_docs) if i not in all_duplicates]
    }


if __name__ == '__main__':
    # Test
    test_texts = [
        "VÄƒn báº£n 1",
        "VÄƒn báº£n 1 sá»­a Ä‘á»•i",
        "VÄƒn báº£n 2",
        "VÄƒn báº£n 3"
    ]
    
    test_pairs = [(0, 1, 0.95), (2, 3, 0.88)]
    
    result = process_clustering(test_pairs, test_texts, representative_method='shortest')
    print(f"\nClusters: {len(result['clusters'])}")