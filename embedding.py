"""
Module táº¡o embeddings tá»« text sá»­ dá»¥ng SentenceTransformers
"""
import numpy as np
from sentence_transformers import SentenceTransformer
import torch


class TextEmbedder:
    """Class Ä‘á»ƒ táº¡o embeddings cho vÄƒn báº£n"""
    
    _instance = None
    
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        """
        Khá»Ÿi táº¡o model embedding (singleton pattern)
        
        Args:
            model_name: TÃªn model tá»« sentence-transformers
        """
        if TextEmbedder._instance is not None:
            self.model = TextEmbedder._instance.model
            self.model_name = TextEmbedder._instance.model_name
            return
        
        print(f"ğŸ“¦ Äang táº£i model {model_name}...")
        
        # Thiáº¿t láº­p device
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"   Device: {device}")
        
        self.model = SentenceTransformer(model_name, device=device)
        self.model_name = model_name
        
        TextEmbedder._instance = self
        print(f"âœ“ Model {model_name} Ä‘Ã£ sáºµn sÃ ng")
    
    def embed_texts(self, texts, batch_size=64):
        """
        Táº¡o embeddings cho danh sÃ¡ch vÄƒn báº£n
        
        Args:
            texts: List cÃ¡c vÄƒn báº£n (string)
            batch_size: Sá»‘ vÄƒn báº£n xá»­ lÃ½ má»—i láº§n
        
        Returns:
            numpy array shape (n_texts, embedding_dim) - dtype float32
        """
        if not texts:
            raise ValueError("Danh sÃ¡ch vÄƒn báº£n rá»—ng")
        
        if len(texts) > 10000:
            print(f"âš ï¸  Cáº£nh bÃ¡o: Embedding {len(texts)} vÄƒn báº£n cÃ³ thá»ƒ máº¥t nhiá»u thá»i gian")
        
        print(f"ğŸ“Š Táº¡o embeddings cho {len(texts)} vÄƒn báº£n (batch_size={batch_size})...")
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=False  # Normalize sau trong FAISS
        )
        
        embeddings = embeddings.astype(np.float32)
        
        print(f"âœ“ HoÃ n táº¥t. Shape: {embeddings.shape}")
        return embeddings
    
    def get_embedding_dim(self):
        """Láº¥y chiá»u cá»§a embedding vector"""
        return self.model.get_sentence_embedding_dimension()


def get_embeddings_from_texts(texts: list, model_name='all-MiniLM-L6-v2', batch_size=64) -> np.ndarray:
    """
    Function tiá»‡n Ã­ch Ä‘á»ƒ táº¡o embeddings tá»« danh sÃ¡ch text
    
    Args:
        texts: List cÃ¡c vÄƒn báº£n string
        model_name: TÃªn model SentenceTransformer
        batch_size: KÃ­ch thÆ°á»›c batch
    
    Returns:
        numpy array embeddings (float32)
    """
    embedder = TextEmbedder(model_name)
    return embedder.embed_texts(texts, batch_size)


if __name__ == '__main__':
    # Test
    test_texts = [
        "Viá»‡t Nam lÃ  má»™t nÆ°á»›c xÃ£ há»™i chá»§ nghÄ©a",
        "Viá»‡t Nam lÃ  má»™t nÆ°á»›c cÃ³ thá»§ Ä‘Ã´ HÃ  Ná»™i",
        "Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh phá»• biáº¿n"
    ]
    
    embeddings = get_embeddings_from_texts(test_texts)
    print(f"\nTest embeddings shape: {embeddings.shape}")
    print(f"Type: {type(embeddings)}, dtype: {embeddings.dtype}")